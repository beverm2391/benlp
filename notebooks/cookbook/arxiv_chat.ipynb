{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e67f200",
   "metadata": {},
   "source": [
    "# How to use functions with a knowledge base\n",
    "\n",
    "This notebook builds on the concepts in the [argument generation](How_to_call_functions_with_chat_models.ipynb) notebook, by creating an agent with access to a knowledge base and two functions that it can call based on the user requirement.\n",
    "\n",
    "We'll create an agent that uses data from arXiv to answer questions about academic subjects. It has two functions at its disposal:\n",
    "- **get_articles**: A function that gets arXiv articles on a subject and summarizes them for the user with links.\n",
    "- **read_article_and_summarize**: This function takes one of the previously searched articles, reads it in its entirety and summarizes the core argument, evidence and conclusions.\n",
    "\n",
    "This will get you comfortable with a multi-function workflow that can choose from multiple services, and where some of the data from the first function is persisted to be used by the second.\n",
    "\n",
    "## Walkthrough\n",
    "\n",
    "This cookbook takes you through the following workflow:\n",
    "\n",
    "- **Search utilities:** Creating the two functions that access arXiv for answers.\n",
    "- **Configure Agent:** Building up the Agent behaviour that will assess the need for a function and, if one is required, call that function and present results back to the agent.\n",
    "- **arXiv conversation:** Put all of this together in live conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import ast\n",
    "import concurrent\n",
    "from csv import writer\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "from scipy import spatial\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import time\n",
    "import random\n",
    "\n",
    "from benlp.tools.code_executor import execute_code\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2e47962",
   "metadata": {},
   "source": [
    "## Search utilities\n",
    "\n",
    "We'll first set up some utilities that will underpin our two functions.\n",
    "\n",
    "Downloaded papers will be stored in a directory (we use ```./data/papers``` here). We create a file ```arxiv_library.csv``` to store the embeddings and details for downloaded papers to retrieve against using ```summarize_text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de5d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a directory to store downloaded papers\n",
    "# data_dir = os.path.join(os.curdir, \"data\", \"test\", \"papers\")\n",
    "data_dir = \"../../data/test/papers\"\n",
    "paper_dir_filepath = \"../../data/test/arxiv_library.csv\"\n",
    "\n",
    "# Generate a blank dataframe where we can store downloaded files\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(paper_dir_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57217b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def embedding_request(text):\n",
    "    response = openai.Embedding.create(input=text, model=EMBEDDING_MODEL)\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
    "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
    "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
    "    \"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=query, max_results=top_k, sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    result_list = []\n",
    "    for result in search.results():\n",
    "        result_dict = {}\n",
    "        result_dict.update({\"title\": result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "\n",
    "        # Taking the first url provided\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "        # Store references in library file\n",
    "        response = embedding_request(text=result.title)\n",
    "        file_reference = [\n",
    "            result.title,\n",
    "            result.download_pdf(data_dir),\n",
    "            response[\"data\"][0][\"embedding\"],\n",
    "        ]\n",
    "\n",
    "        # Write to file\n",
    "        with open(library, \"a\") as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda02bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Riemannian Proximal Policy Optimization',\n",
       " 'summary': 'In this paper, We propose a general Riemannian proximal optimization\\nalgorithm with guaranteed convergence to solve Markov decision process (MDP)\\nproblems. To model policy functions in MDP, we employ Gaussian mixture model\\n(GMM) and formulate it as a nonconvex optimization problem in the Riemannian\\nspace of positive semidefinite matrices. For two given policy functions, we\\nalso provide its lower bound on policy improvement by using bounds derived from\\nthe Wasserstein distance of GMMs. Preliminary experiments show the efficacy of\\nour proposed Riemannian proximal policy optimization algorithm.',\n",
       " 'article_url': 'http://arxiv.org/abs/2005.09195v1',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2005.09195v1'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the search is working\n",
    "result_output = get_articles(\"Proximal Policy Optimization\")\n",
    "result_output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2abd0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Riemannian Proximal Policy Optimization',\n",
       "  'summary': 'In this paper, We propose a general Riemannian proximal optimization\\nalgorithm with guaranteed convergence to solve Markov decision process (MDP)\\nproblems. To model policy functions in MDP, we employ Gaussian mixture model\\n(GMM) and formulate it as a nonconvex optimization problem in the Riemannian\\nspace of positive semidefinite matrices. For two given policy functions, we\\nalso provide its lower bound on policy improvement by using bounds derived from\\nthe Wasserstein distance of GMMs. Preliminary experiments show the efficacy of\\nour proposed Riemannian proximal policy optimization algorithm.',\n",
       "  'article_url': 'http://arxiv.org/abs/2005.09195v1',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2005.09195v1'},\n",
       " {'title': 'Natural Policy Gradients In Reinforcement Learning Explained',\n",
       "  'summary': 'Traditional policy gradient methods are fundamentally flawed. Natural\\ngradients converge quicker and better, forming the foundation of contemporary\\nReinforcement Learning such as Trust Region Policy Optimization (TRPO) and\\nProximal Policy Optimization (PPO). This lecture note aims to clarify the\\nintuition behind natural policy gradients, focusing on the thought process and\\nthe key mathematical constructs.',\n",
       "  'article_url': 'http://arxiv.org/abs/2209.01820v1',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2209.01820v1'},\n",
       " {'title': 'Generalized Proximal Policy Optimization with Sample Reuse',\n",
       "  'summary': 'In real-world decision making tasks, it is critical for data-driven\\nreinforcement learning methods to be both stable and sample efficient.\\nOn-policy methods typically generate reliable policy improvement throughout\\ntraining, while off-policy methods make more efficient use of data through\\nsample reuse. In this work, we combine the theoretically supported stability\\nbenefits of on-policy algorithms with the sample efficiency of off-policy\\nalgorithms. We develop policy improvement guarantees that are suitable for the\\noff-policy setting, and connect these bounds to the clipping mechanism used in\\nProximal Policy Optimization. This motivates an off-policy version of the\\npopular algorithm that we call Generalized Proximal Policy Optimization with\\nSample Reuse. We demonstrate both theoretically and empirically that our\\nalgorithm delivers improved performance by effectively balancing the competing\\ngoals of stability and sample efficiency.',\n",
       "  'article_url': 'http://arxiv.org/abs/2111.00072v1',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2111.00072v1'},\n",
       " {'title': 'Proximal Policy Optimization with Mixed Distributed Training',\n",
       "  'summary': 'Instability and slowness are two main problems in deep reinforcement\\nlearning. Even if proximal policy optimization (PPO) is the state of the art,\\nit still suffers from these two problems. We introduce an improved algorithm\\nbased on proximal policy optimization, mixed distributed proximal policy\\noptimization (MDPPO), and show that it can accelerate and stabilize the\\ntraining process. In our algorithm, multiple different policies train\\nsimultaneously and each of them controls several identical agents that interact\\nwith environments. Actions are sampled by each policy separately as usual, but\\nthe trajectories for the training process are collected from all agents,\\ninstead of only one policy. We find that if we choose some auxiliary\\ntrajectories elaborately to train policies, the algorithm will be more stable\\nand quicker to converge especially in the environments with sparse rewards.',\n",
       "  'article_url': 'http://arxiv.org/abs/1907.06479v3',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/1907.06479v3'},\n",
       " {'title': 'Supervised Policy Update for Deep Reinforcement Learning',\n",
       "  'summary': 'We propose a new sample-efficient methodology, called Supervised Policy\\nUpdate (SPU), for deep reinforcement learning. Starting with data generated by\\nthe current policy, SPU formulates and solves a constrained optimization\\nproblem in the non-parameterized proximal policy space. Using supervised\\nregression, it then converts the optimal non-parameterized policy to a\\nparameterized policy, from which it draws new samples. The methodology is\\ngeneral in that it applies to both discrete and continuous action spaces, and\\ncan handle a wide variety of proximity constraints for the non-parameterized\\noptimization problem. We show how the Natural Policy Gradient and Trust Region\\nPolicy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization\\n(PPO) problem can be addressed by this methodology. The SPU implementation is\\nmuch simpler than TRPO. In terms of sample efficiency, our extensive\\nexperiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and\\noutperforms PPO in Atari video game tasks.',\n",
       "  'article_url': 'http://arxiv.org/abs/1805.11706v4',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/1805.11706v4'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11675627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100,\n",
    ") -> list[str]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = embedding_request(query)\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7211df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def extract_chunk(content, template_prompt):\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarize chunk of text\"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarize_text(query):\n",
    "    \"\"\"This function does the following:\n",
    "    - Reads in the arxiv_library.csv file in including the embeddings\n",
    "    - Finds the closest file to the user's query\n",
    "    - Scrapes the text out of the file and chunks it\n",
    "    - Summarizes each chunk in parallel\n",
    "    - Does one final summary and returns this to the user\"\"\"\n",
    "\n",
    "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
    "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
    "\n",
    "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
    "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    if len(library_df) == 0:\n",
    "        print(\"No papers searched yet, downloading first.\")\n",
    "        get_articles(query)\n",
    "        print(\"Papers downloaded, continuing\")\n",
    "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
    "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
    "    print(\"Chunking text from paper\")\n",
    "    pdf_text = read_pdf(strings[0])\n",
    "\n",
    "    # Initialise tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    results = \"\"\n",
    "\n",
    "    # Chunk up the document into 1500 token chunks\n",
    "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    print(\"Summarizing each chunk of text\")\n",
    "\n",
    "    # Parallel process the summaries\n",
    "    with concurrent.futures.ThreadPoolExecutor(\n",
    "        max_workers=len(text_chunks)\n",
    "    ) as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "        with tqdm(total=len(text_chunks)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            results += data\n",
    "\n",
    "    # Final summary\n",
    "    print(\"Summarizing into overall summary\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            # {\n",
    "            #     \"role\": \"user\",\n",
    "            #     \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
    "            #             The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "            #             User query: {query}\n",
    "            #             The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "            #             Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
    "            # }\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\" : f\"\"\"Fulfill the user query to the best of your ability using information extracted from an academic paper.\n",
    "                User query: {query}\n",
    "                Key points:\\n{results}\\nSummary:\\n\n",
    "                Provide appropriate references to the paper in the summary.\n",
    "                \"\"\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "898b94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:11<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    }
   ],
   "source": [
    "# Test the summarize_text function works\n",
    "chat_test_response = summarize_text(\"What are the limitations of Proximal Policy Optimization?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c715f60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The limitations of Proximal Policy Optimization (PPO) are not explicitly mentioned in the provided information. However, the academic paper proposes a Riemannian proximal policy optimization algorithm as an improvement over traditional policy gradient methods. The paper addresses challenges such as high variance, sample inefficiency, and difficulty in tuning learning rate faced by traditional methods. The proposed algorithm leverages manifold learning and offers interpretability and speed of convergence improvements. The paper also provides theoretical analysis and bounds for policy optimization using the Wasserstein distance and total variation distance. The efficacy of the proposed algorithm is demonstrated through preliminary experiments. \n",
      "\n",
      "Reference:\n",
      "[Paper Title] (Provide the actual reference to the academic paper)\n"
     ]
    }
   ],
   "source": [
    "print(chat_test_response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dab07e98",
   "metadata": {},
   "source": [
    "## Configure Agent\n",
    "\n",
    "We'll create our agent in this step, including a ```Conversation``` class to support multiple turns with the API, and some Python functions to enable interaction between the ```ChatCompletion``` API and our knowledge base functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77a6fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, functions=None, model=GPT_MODEL):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
    "    }\n",
    "    json_data = {\"model\": model, \"messages\": messages}\n",
    "    if functions is not None:\n",
    "        json_data.update({\"functions\": functions})\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_data,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73f7672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        message = {\"role\": role, \"content\": content}\n",
    "        self.conversation_history.append(message)\n",
    "\n",
    "    def display_conversation(self, detailed=False):\n",
    "        role_to_color = {\n",
    "            \"system\": \"red\",\n",
    "            \"user\": \"green\",\n",
    "            \"assistant\": \"blue\",\n",
    "            \"function\": \"magenta\",\n",
    "        }\n",
    "        for message in self.conversation_history:\n",
    "            print(\n",
    "                colored(\n",
    "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
    "                    role_to_color[message[\"role\"]],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "978b7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate our get_articles and read_article_and_summarize functions\n",
    "arxiv_functions = [\n",
    "    {\n",
    "        \"name\": \"get_articles\",\n",
    "        \"description\": \"\"\"Use this function to get academic papers from arXiv to answer user questions.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            User query in JSON. Responses should be summarized and should include the article URL reference\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "        \"name\": \"read_article_and_summarize\",\n",
    "        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n",
    "        You should NEVER call this function before get_articles has been called in the conversation.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            Description of the article in plain text based on the user's query\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "        \"name\" : \"execute_code\",\n",
    "        \"description\" : \"\"\"Use this function to execute python code.\"\"\",\n",
    "        \"parameters\" : {\n",
    "            \"type\" : \"object\",\n",
    "            \"properties\" : {\n",
    "                \"code\" : {\n",
    "                    \"type\" : \"string\",\n",
    "                    \"description\" : f\"\"\"\n",
    "                            Python code to be executed\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\" : [\"code\"],\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12edbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_parse(json_str : str, attempts=3):\n",
    "    print(\"Attemps Left: \", attempts)\n",
    "    json_str = json_str.strip()\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            parsed = json.loads(json_str)\n",
    "            if isinstance(parsed, str):\n",
    "                # If the parsed object is a string, try to parse it again\n",
    "                return json.loads(parsed)\n",
    "            else:\n",
    "                return parsed\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Unable to parse JSON:\\n {json_str}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            print(\"modifying JSON and retrying\")\n",
    "\n",
    "            # Handle Python to JSON value replacements\n",
    "            replacements = {\n",
    "                \"None\": \"null\",\n",
    "                \"True\": \"true\",\n",
    "                \"False\": \"false\"\n",
    "            }\n",
    "            for py_val, json_val in replacements.items():\n",
    "                if py_val in json_str:\n",
    "                    json_str = json_str.replace(py_val, json_val)\n",
    "\n",
    "            # Handle unescaped control characters\n",
    "            if \"Invalid control character\" in str(e):\n",
    "                json_str = json.dumps(json_str)\n",
    "        except Exception as e:\n",
    "            # For other exceptions, just print the error and retry\n",
    "            print(f\"Exception: {e}\")\n",
    "\n",
    "    # If all attempts fail, raise the last exception\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c88ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_with_function_execution(messages, functions=[None]):\n",
    "    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n",
    "    response = chat_completion_request(messages, functions)\n",
    "    full_message = response.json()[\"choices\"][0]\n",
    "    if full_message[\"finish_reason\"] == \"function_call\":\n",
    "        print(f\"Function generation requested, calling function\")\n",
    "        return call_arxiv_function(messages, full_message)\n",
    "    else:\n",
    "        print(f\"Function not required, responding to user\")\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "def call_arxiv_function(messages, full_message):\n",
    "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
    "    Currently extended by adding clauses to this if statement.\"\"\"\n",
    "\n",
    "    if full_message[\"message\"][\"function_call\"][\"name\"] == \"get_articles\":\n",
    "        try:\n",
    "            parsed_output = json.loads(\n",
    "                full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "            )\n",
    "            print(\"Getting search results\")\n",
    "            results = get_articles(parsed_output[\"query\"])\n",
    "        except Exception as e:\n",
    "            print(parsed_output)\n",
    "            print(f\"Function execution failed\")\n",
    "            print(f\"Error message: {e}\")\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": full_message[\"message\"][\"function_call\"][\"name\"],\n",
    "                \"content\": str(results),\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            print(\"Got search results, summarizing content\")\n",
    "            response = chat_completion_request(messages)\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(type(e))\n",
    "            raise Exception(\"Function chat request failed\")\n",
    "\n",
    "    elif (\n",
    "        full_message[\"message\"][\"function_call\"][\"name\"] == \"read_article_and_summarize\"\n",
    "    ):\n",
    "        parsed_output = json.loads(\n",
    "            full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "        )\n",
    "        print(\"Finding and reading paper\")\n",
    "        summary = summarize_text(parsed_output[\"query\"])\n",
    "        return summary\n",
    "    \n",
    "    elif (\n",
    "        full_message[\"message\"][\"function_call\"][\"name\"] == \"execute_code\"\n",
    "    ):\n",
    "        args = full_message[\"message\"][\"function_call\"][\"arguments\"].strip()\n",
    "        parsed_output = dynamic_parse(args)\n",
    "        print(\"Executing python code\")\n",
    "        result = execute_code(parsed_output[\"code\"])\n",
    "        print(\"Code executed\")\n",
    "        print(result)\n",
    "        return result\n",
    "\n",
    "    else:\n",
    "        raise Exception(f'Function {full_message[\"message\"][\"function_call\"][\"name\"]} does not exist and cannot be called')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd3e7868",
   "metadata": {},
   "source": [
    "## arXiv conversation\n",
    "\n",
    "Let's put this all together by testing our functions out in conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c39a1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system message\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
    "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
    "Begin!\"\"\"\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253fd0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function not required, responding to user\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "There are several factors that can affect the performance of a reinforcement learning (RL) model after fine-tuning. The size of the RL model is one such factor. Larger models tend to have more capacity to capture complex patterns and representations, which can potentially improve performance. However, the size of the model also affects training and inference time, as well as the amount of computational resources required.\n",
       "\n",
       "One paper that investigates the impact of model size on RL performance is \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\" by Jonathan Frankle and Michael Carbin. In this paper, the authors propose the \"lottery ticket hypothesis\" which suggests that large neural networks contain a small subnetwork that can achieve comparable performance to the full-sized network when trained in isolation. The authors demonstrate this hypothesis on several tasks, including reinforcement learning.\n",
       "\n",
       "Another relevant paper is \"Impact of Model Size on Deep Reinforcement Learning\" by Tom Schaul et al. The authors investigate the effect of model size on the performance of deep RL agents across various tasks. They find that increasing model size improves performance up to a certain point, after which performance plateaus or even deteriorates. The authors attribute this phenomenon to overfitting and emphasize the importance of regularizing large models.\n",
       "\n",
       "Overall, while increasing the size of an RL model can potentially lead to performance improvements, it is important to consider the trade-off between model size, computational resources, training time, and overfitting. It is recommended to experiment with different model sizes and monitor performance to determine the optimal size for a specific task. \n",
       "\n",
       "Here are the details of the papers I mentioned:\n",
       "\n",
       "1. \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\"\n",
       "   - Article URL: [arxiv.org/abs/1803.03635](https://arxiv.org/abs/1803.03635)\n",
       "   - Authors: Jonathan Frankle and Michael Carbin\n",
       "\n",
       "2. \"Impact of Model Size on Deep Reinforcement Learning\"\n",
       "   - Article URL: [arxiv.org/abs/2004.07219](https://arxiv.org/abs/2004.07219)\n",
       "   - Authors: Tom Schaul, et al."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a user message\n",
    "paper_conversation.add_message(\"user\", \"Hi, how does the size of the RL model affect the performance after renforcement tuing\")\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ca3e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function\n",
      "Attemps Left:  3\n",
      "Executing python code\n",
      "Code executed\n",
      "Error: name 'log' is not defined\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m paper_conversation\u001b[39m.\u001b[39madd_message(\n\u001b[1;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCalculate the result of black scholes for stock price 100, strike price 95, time to maturity 0.5, risk free rate 0.05, volatility 0.2\u001b[39m\u001b[39m\"\u001b[39m,  \n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m updated_response \u001b[39m=\u001b[39m chat_completion_with_function_execution(\n\u001b[1;32m      7\u001b[0m     paper_conversation\u001b[39m.\u001b[39mconversation_history, functions\u001b[39m=\u001b[39marxiv_functions\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m display(Markdown(updated_response[\u001b[39m\"\u001b[39;49m\u001b[39mchoices\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# Add another user message to induce our system to use the second tool\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"Calculate the result of black scholes for stock price 100, strike price 95, time to maturity 0.5, risk free rate 0.05, volatility 0.2\",  \n",
    ")\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "display(Markdown(updated_response[\"choices\"][0][\"message\"][\"content\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
